{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# \ucf54\ub4dc \ub0b4\ubd80\uc5d0 \ud55c\uae00\uc744 \uc0ac\uc6a9\uac00\ub2a5 \ud558\uac8c \ud574\uc8fc\ub294 \ubd80\ubd84\uc785\ub2c8\ub2e4.\n",
        "\n",
        "# \ub85c\uc774\ud130 \ub274\uc2a4 \ub370\uc774\ud130\uc14b \ubd88\ub7ec\uc624\uae30\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# seed \uac12 \uc124\uc815\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# \ubd88\ub7ec\uc628 \ub370\uc774\ud130\ub97c \ud559\uc2b5\uc14b, \ud14c\uc2a4\ud2b8\uc14b\uc73c\ub85c \ub098\ub204\uae30\n",
        "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words=1000, test_split=0.2)\n",
        "\n",
        "# \ub370\uc774\ud130 \ud655\uc778\ud558\uae30\n",
        "category = numpy.max(Y_train) + 1\n",
        "print(category, '\uce74\ud14c\uace0\ub9ac')\n",
        "print(len(X_train), '\ud559\uc2b5\uc6a9 \ub274\uc2a4 \uae30\uc0ac')\n",
        "print(len(X_test), '\ud14c\uc2a4\ud2b8\uc6a9 \ub274\uc2a4 \uae30\uc0ac')\n",
        "print(X_train[0])\n",
        "\n",
        "# \ub370\uc774\ud130 \uc804\ucc98\ub9ac\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=100)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=100)\n",
        "y_train = np_utils.to_categorical(Y_train)\n",
        "y_test = np_utils.to_categorical(Y_test)\n",
        "\n",
        "# \ubaa8\ub378\uc758 \uc124\uc815\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, 100))\n",
        "model.add(LSTM(100, activation='tanh'))\n",
        "model.add(Dense(46, activation='softmax'))\n",
        "\n",
        "# \ubaa8\ub378\uc758 \ucef4\ud30c\uc77c\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer='adam',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "# \ubaa8\ub378\uc758 \uc2e4\ud589\n",
        "history = model.fit(x_train, y_train, batch_size=100, epochs=20, validation_data=(x_test, y_test))\n",
        "\n",
        "# \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4 \ucd9c\ub825\n",
        "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))\n",
        "\n",
        "\n",
        "# \ud14c\uc2a4\ud2b8 \uc14b\uc758 \uc624\ucc28\n",
        "y_vloss = history.history['val_loss']\n",
        "\n",
        "# \ud559\uc2b5\uc14b\uc758 \uc624\ucc28\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "# \uadf8\ub798\ud504\ub85c \ud45c\ud604\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
        "\n",
        "# \uadf8\ub798\ud504\uc5d0 \uadf8\ub9ac\ub4dc\ub97c \uc8fc\uace0 \ub808\uc774\ube14\uc744 \ud45c\uc2dc\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}