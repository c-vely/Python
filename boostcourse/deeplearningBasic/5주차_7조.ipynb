{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24084154",
   "metadata": {},
   "source": [
    "## Q1. MNIST 데이터셋 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba88c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_epochs = 15 # training 반복 횟수\n",
    "batch_size = 100\n",
    "\n",
    "root = './data'\n",
    "mnist_train = dset.MNIST (root=root, train=True, download=True, \n",
    "                          transform=transforms.Compose([transforms.ToTensor()])) \n",
    "# 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환하는 코드 추가(Q4에서 읽지 못 해서...)\n",
    "mnist_test = dset.MNIST (root=root, train=False, download=True, \n",
    "                          transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# data loader를 직접 구현해보자.\n",
    "train_loader = DataLoader(mnist_train, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7e0f7",
   "metadata": {},
   "source": [
    "## Q2. 가중치 초기화, linear한 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6837c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4138,  0.5833,  0.1070,  ..., -0.7771, -0.0995, -0.1704],\n",
       "        [ 0.0935,  0.1986, -1.1416,  ...,  0.2281,  0.7082, -0.2708],\n",
       "        [ 0.8426, -0.1350, -0.0634,  ..., -0.3347, -1.2321, -0.1975],\n",
       "        ...,\n",
       "        [-1.1458,  0.6704,  1.1703,  ...,  1.0139, -0.2122,  0.1631],\n",
       "        [ 0.4777,  0.2217, -1.3957,  ..., -1.2683, -0.9370, -0.8959],\n",
       "        [-1.7948,  0.1322, -1.2780,  ..., -0.0873, -0.5200,  1.5012]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "linear = nn.Linear(784, 10, bias = True).to(device)\n",
    "nn.init.normal_(linear.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa638b",
   "metadata": {},
   "source": [
    "## Q3. loss 함수와 optimizer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f2e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn - Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# optimizer - SGD\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b64fcb",
   "metadata": {},
   "source": [
    "## Q4. 학습 Loop 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53c4a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [100/600], Loss:  4.5266, Accuracy:  40.00%\n",
      "Epoch [1/15], Step [200/600], Loss:  2.5137, Accuracy:  55.00%\n",
      "Epoch [1/15], Step [300/600], Loss:  1.8212, Accuracy:  72.00%\n",
      "Epoch [1/15], Step [400/600], Loss:  1.8822, Accuracy:  69.00%\n",
      "Epoch [1/15], Step [500/600], Loss:  1.0698, Accuracy:  76.00%\n",
      "Epoch [1/15], Step [600/600], Loss:  1.1405, Accuracy:  78.00%\n",
      "Epoch [2/15], Step [100/600], Loss:  1.2581, Accuracy:  75.00%\n",
      "Epoch [2/15], Step [200/600], Loss:  0.8003, Accuracy:  85.00%\n",
      "Epoch [2/15], Step [300/600], Loss:  0.8957, Accuracy:  81.00%\n",
      "Epoch [2/15], Step [400/600], Loss:  0.5392, Accuracy:  88.00%\n",
      "Epoch [2/15], Step [500/600], Loss:  0.9362, Accuracy:  78.00%\n",
      "Epoch [2/15], Step [600/600], Loss:  1.2277, Accuracy:  73.00%\n",
      "Epoch [3/15], Step [100/600], Loss:  1.0268, Accuracy:  80.00%\n",
      "Epoch [3/15], Step [200/600], Loss:  1.2081, Accuracy:  81.00%\n",
      "Epoch [3/15], Step [300/600], Loss:  0.7259, Accuracy:  83.00%\n",
      "Epoch [3/15], Step [400/600], Loss:  0.7100, Accuracy:  84.00%\n",
      "Epoch [3/15], Step [500/600], Loss:  0.8791, Accuracy:  87.00%\n",
      "Epoch [3/15], Step [600/600], Loss:  0.9555, Accuracy:  80.00%\n",
      "Epoch [4/15], Step [100/600], Loss:  0.4650, Accuracy:  88.00%\n",
      "Epoch [4/15], Step [200/600], Loss:  1.1122, Accuracy:  82.00%\n",
      "Epoch [4/15], Step [300/600], Loss:  0.8726, Accuracy:  85.00%\n",
      "Epoch [4/15], Step [400/600], Loss:  1.3357, Accuracy:  81.00%\n",
      "Epoch [4/15], Step [500/600], Loss:  1.0891, Accuracy:  80.00%\n",
      "Epoch [4/15], Step [600/600], Loss:  0.7890, Accuracy:  83.00%\n",
      "Epoch [5/15], Step [100/600], Loss:  0.7584, Accuracy:  83.00%\n",
      "Epoch [5/15], Step [200/600], Loss:  0.3642, Accuracy:  90.00%\n",
      "Epoch [5/15], Step [300/600], Loss:  0.7746, Accuracy:  79.00%\n",
      "Epoch [5/15], Step [400/600], Loss:  0.5771, Accuracy:  86.00%\n",
      "Epoch [5/15], Step [500/600], Loss:  0.5762, Accuracy:  90.00%\n",
      "Epoch [5/15], Step [600/600], Loss:  0.7145, Accuracy:  84.00%\n",
      "Epoch [6/15], Step [100/600], Loss:  0.3130, Accuracy:  90.00%\n",
      "Epoch [6/15], Step [200/600], Loss:  0.9418, Accuracy:  81.00%\n",
      "Epoch [6/15], Step [300/600], Loss:  0.8890, Accuracy:  83.00%\n",
      "Epoch [6/15], Step [400/600], Loss:  0.7715, Accuracy:  81.00%\n",
      "Epoch [6/15], Step [500/600], Loss:  1.2025, Accuracy:  77.00%\n",
      "Epoch [6/15], Step [600/600], Loss:  0.6065, Accuracy:  87.00%\n",
      "Epoch [7/15], Step [100/600], Loss:  0.3139, Accuracy:  87.00%\n",
      "Epoch [7/15], Step [200/600], Loss:  0.8218, Accuracy:  85.00%\n",
      "Epoch [7/15], Step [300/600], Loss:  0.5296, Accuracy:  88.00%\n",
      "Epoch [7/15], Step [400/600], Loss:  0.5062, Accuracy:  91.00%\n",
      "Epoch [7/15], Step [500/600], Loss:  0.5793, Accuracy:  88.00%\n",
      "Epoch [7/15], Step [600/600], Loss:  1.1864, Accuracy:  86.00%\n",
      "Epoch [8/15], Step [100/600], Loss:  0.6685, Accuracy:  89.00%\n",
      "Epoch [8/15], Step [200/600], Loss:  1.2081, Accuracy:  77.00%\n",
      "Epoch [8/15], Step [300/600], Loss:  0.3754, Accuracy:  84.00%\n",
      "Epoch [8/15], Step [400/600], Loss:  0.5873, Accuracy:  88.00%\n",
      "Epoch [8/15], Step [500/600], Loss:  0.4150, Accuracy:  92.00%\n",
      "Epoch [8/15], Step [600/600], Loss:  0.6536, Accuracy:  89.00%\n",
      "Epoch [9/15], Step [100/600], Loss:  0.5751, Accuracy:  85.00%\n",
      "Epoch [9/15], Step [200/600], Loss:  0.2228, Accuracy:  96.00%\n",
      "Epoch [9/15], Step [300/600], Loss:  0.4710, Accuracy:  87.00%\n",
      "Epoch [9/15], Step [400/600], Loss:  0.6969, Accuracy:  86.00%\n",
      "Epoch [9/15], Step [500/600], Loss:  0.5520, Accuracy:  87.00%\n",
      "Epoch [9/15], Step [600/600], Loss:  0.5319, Accuracy:  85.00%\n",
      "Epoch [10/15], Step [100/600], Loss:  0.3997, Accuracy:  89.00%\n",
      "Epoch [10/15], Step [200/600], Loss:  0.2627, Accuracy:  91.00%\n",
      "Epoch [10/15], Step [300/600], Loss:  0.6645, Accuracy:  88.00%\n",
      "Epoch [10/15], Step [400/600], Loss:  0.2507, Accuracy:  92.00%\n",
      "Epoch [10/15], Step [500/600], Loss:  0.4992, Accuracy:  87.00%\n",
      "Epoch [10/15], Step [600/600], Loss:  0.4702, Accuracy:  88.00%\n",
      "Epoch [11/15], Step [100/600], Loss:  0.5162, Accuracy:  90.00%\n",
      "Epoch [11/15], Step [200/600], Loss:  0.3509, Accuracy:  87.00%\n",
      "Epoch [11/15], Step [300/600], Loss:  0.3292, Accuracy:  88.00%\n",
      "Epoch [11/15], Step [400/600], Loss:  0.5104, Accuracy:  88.00%\n",
      "Epoch [11/15], Step [500/600], Loss:  0.5174, Accuracy:  90.00%\n",
      "Epoch [11/15], Step [600/600], Loss:  0.7684, Accuracy:  85.00%\n",
      "Epoch [12/15], Step [100/600], Loss:  0.7403, Accuracy:  89.00%\n",
      "Epoch [12/15], Step [200/600], Loss:  0.5969, Accuracy:  85.00%\n",
      "Epoch [12/15], Step [300/600], Loss:  0.4018, Accuracy:  85.00%\n",
      "Epoch [12/15], Step [400/600], Loss:  0.3324, Accuracy:  89.00%\n",
      "Epoch [12/15], Step [500/600], Loss:  0.3868, Accuracy:  90.00%\n",
      "Epoch [12/15], Step [600/600], Loss:  0.6623, Accuracy:  87.00%\n",
      "Epoch [13/15], Step [100/600], Loss:  0.4306, Accuracy:  92.00%\n",
      "Epoch [13/15], Step [200/600], Loss:  0.6964, Accuracy:  86.00%\n",
      "Epoch [13/15], Step [300/600], Loss:  0.3822, Accuracy:  89.00%\n",
      "Epoch [13/15], Step [400/600], Loss:  0.6359, Accuracy:  89.00%\n",
      "Epoch [13/15], Step [500/600], Loss:  0.5030, Accuracy:  87.00%\n",
      "Epoch [13/15], Step [600/600], Loss:  0.4076, Accuracy:  90.00%\n",
      "Epoch [14/15], Step [100/600], Loss:  1.0685, Accuracy:  85.00%\n",
      "Epoch [14/15], Step [200/600], Loss:  0.4289, Accuracy:  91.00%\n",
      "Epoch [14/15], Step [300/600], Loss:  0.5289, Accuracy:  84.00%\n",
      "Epoch [14/15], Step [400/600], Loss:  0.4552, Accuracy:  86.00%\n",
      "Epoch [14/15], Step [500/600], Loss:  0.6839, Accuracy:  88.00%\n",
      "Epoch [14/15], Step [600/600], Loss:  0.5780, Accuracy:  88.00%\n",
      "Epoch [15/15], Step [100/600], Loss:  0.4078, Accuracy:  90.00%\n",
      "Epoch [15/15], Step [200/600], Loss:  0.2778, Accuracy:  89.00%\n",
      "Epoch [15/15], Step [300/600], Loss:  0.3766, Accuracy:  91.00%\n",
      "Epoch [15/15], Step [400/600], Loss:  0.2877, Accuracy:  91.00%\n",
      "Epoch [15/15], Step [500/600], Loss:  0.6565, Accuracy:  89.00%\n",
      "Epoch [15/15], Step [600/600], Loss:  0.4469, Accuracy:  91.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28 * 28)\n",
    "        \n",
    "        outputs = linear(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,argmax = torch.max(outputs, 1)\n",
    "        accuracy = (labels == argmax).float().mean()\n",
    " \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accuracy: {: .2f}%'.format(epoch+1, \n",
    "                        training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()* 100))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef7e40",
   "metadata": {},
   "source": [
    "## Q5. 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463a1337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 10000 images:  88.82%\n"
     ]
    }
   ],
   "source": [
    "linear.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28 * 28)\n",
    "\n",
    "        outputs = linear(imgs) # 테스트 이미지를 선형 모델로 변형\n",
    "\n",
    "        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n",
    "        total += imgs.size(0)\n",
    "        correct += (labels == argmax). sum().item()\n",
    "\n",
    "    print('Test accuracy for {} images: {: .2f}%'.format(total, correct / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b62bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
