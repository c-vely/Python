{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e648c77b",
   "metadata": {},
   "source": [
    "### ğŸ“ŒQ1. ê°€ì¥ ë¨¼ì € í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. MNIST ë°ì´í„°ì…‹ì„ ì§ì ‘ Loadí•´ ë´…ì‹œë‹¤. ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  DataLoaderë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”.\n",
    "\n",
    "* DataLoaderë¥¼ ì´ìš©í•´ MNIST ë°ì´í„°ì…‹ì„ ë¡œë“œí•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16e95b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac8b083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2ffd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ace5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fedefd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dset.MNIST (root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dset.MNIST (root=root, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a6409438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader êµ¬í˜„\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e07129",
   "metadata": {},
   "source": [
    "### ğŸ“ŒQ2.ë°ì´í„°ê°€ ì¤€ë¹„ê°€ ë˜ì—ˆë‹¤ë©´, ì´ì œ ê·¸ ë°ì´í„°ë¥¼ í•™ìŠµí•  ëª¨ë¸ì„ êµ¬í˜„í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ê·¸í›„ ëª¨ë¸ ì•ˆì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”ì‹œì¼œë³´ì„¸ìš”. ì…ë ¥ ë°ì´í„° í˜•íƒœì— ë§ë„ë¡ linearí•œ ëª¨ë¸ì„ êµ¬ì„±í•´ë³´ì„¸ìš”.\n",
    "\n",
    "* MNIST ì…ë ¥ì˜ í¬ê¸°ëŠ” 28x28ì…ë‹ˆë‹¤.\n",
    "\n",
    "* ì—¬ê¸°ì„œ êµ¬í˜„í•˜ëŠ” linear ëª¨ë¸ì€ ì…ë ¥ì´ 1ì°¨ì›ì´ê¸° ë•Œë¬¸ì— ì…ë ¥ ì°¨ì›ì„ ë§ì¶°ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b000e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10f3ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11776\\2238913059.py:2: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(linear.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1963,  0.6151, -0.4301,  ...,  0.1879, -0.8384, -0.2137],\n",
       "        [-0.0841,  1.1740, -1.1685,  ..., -0.0383,  0.1817,  0.1730],\n",
       "        [ 0.5375, -0.8526, -0.8004,  ..., -0.8893, -0.2328, -0.5297],\n",
       "        ...,\n",
       "        [ 1.4213, -0.6979,  0.2195,  ...,  1.0020, -0.4909,  0.2429],\n",
       "        [ 0.5657, -0.1137, -0.1228,  ..., -0.2837, -0.3430,  0.0146],\n",
       "        [-0.0469, -1.4833, -0.9512,  ..., -0.4798,  1.5152,  0.4757]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization\n",
    "torch.nn.init.normal(linear.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1da53e",
   "metadata": {},
   "source": [
    "### ğŸ“ŒQ3. ìœ„ì—ì„œ êµ¬í˜„í•œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” loss í•¨ìˆ˜ì™€ opitmizerê°€ í•„ìš”í•©ë‹ˆë‹¤. ì•„ë˜ ì œì‹œëœ loss í•¨ìˆ˜ì™€ optimizerë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”. Loss í•¨ìˆ˜ì™€ optimizerëŠ” ëª¨ë¸ ì•ˆì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "* ì˜µí‹°ë§ˆì´ì €ëŠ” SGD, LossëŠ” Cross Entropy Lossë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10f651ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn - Cross Entropy Loss\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bcbd8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer - SGD\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc202a",
   "metadata": {},
   "source": [
    "### ğŸ“ŒQ4. 3ë²ˆ ë¬¸ì œê¹Œì§€ í•´ê²°í•˜ì…¨ë‹¤ë©´, ì´ì œ í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„ëŠ” ê±°ì˜ ëë‚¬ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ êµ¬í˜„ í•¨ìˆ˜ë“¤ì„ ì´ìš©í•´ í•™ìŠµ Loopë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”.\n",
    "\n",
    "* ìœ„ì—ì„œ êµ¬í˜„í•œ ëª¨ë¸, optimzer, loss fn ë“±ì„ ì´ìš©í•´ í•™ìŠµì„ êµ¬í˜„í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "763a5cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step[600/600], Loss: 0.9098, Accuracy: 77.00%\n",
      "Epoch [2/15], Step[600/600], Loss: 0.8568, Accuracy: 82.00%\n",
      "Epoch [3/15], Step[600/600], Loss: 0.9429, Accuracy: 84.00%\n",
      "Epoch [4/15], Step[600/600], Loss: 0.4608, Accuracy: 91.00%\n",
      "Epoch [5/15], Step[600/600], Loss: 0.7316, Accuracy: 84.00%\n",
      "Epoch [6/15], Step[600/600], Loss: 0.5179, Accuracy: 89.00%\n",
      "Epoch [7/15], Step[600/600], Loss: 0.4712, Accuracy: 87.00%\n",
      "Epoch [8/15], Step[600/600], Loss: 0.3044, Accuracy: 92.00%\n",
      "Epoch [9/15], Step[600/600], Loss: 0.4762, Accuracy: 90.00%\n",
      "Epoch [10/15], Step[600/600], Loss: 0.5087, Accuracy: 90.00%\n",
      "Epoch [11/15], Step[600/600], Loss: 0.5068, Accuracy: 91.00%\n",
      "Epoch [12/15], Step[600/600], Loss: 0.3205, Accuracy: 90.00%\n",
      "Epoch [13/15], Step[600/600], Loss: 0.7154, Accuracy: 83.00%\n",
      "Epoch [14/15], Step[600/600], Loss: 0.5065, Accuracy: 90.00%\n",
      "Epoch [15/15], Step[600/600], Loss: 0.2553, Accuracy: 94.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28 * 28)\n",
    "        \n",
    "        outputs = linear(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,argmax = torch.max(outputs, 1)\n",
    "        accuracy = (labels == argmax).float().mean()\n",
    "        \n",
    "    if(i+1)%100 == 0:\n",
    "        print('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e5765",
   "metadata": {},
   "source": [
    "### ğŸ“ŒQ5. í•™ìŠµì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì´ ì˜ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„°ë¡œë“œ íŒŒíŠ¸ì—ì„œ ì¤€ë¹„í–ˆë˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•´ë´…ì‹œë‹¤. ì•„ë˜ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì™„ì„±í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84c0431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 100 images: 92.00%\n",
      "Test accuracy for 200 images: 89.50%\n",
      "Test accuracy for 300 images: 90.33%\n",
      "Test accuracy for 400 images: 88.50%\n",
      "Test accuracy for 500 images: 87.80%\n",
      "Test accuracy for 600 images: 86.67%\n",
      "Test accuracy for 700 images: 87.14%\n",
      "Test accuracy for 800 images: 87.25%\n",
      "Test accuracy for 900 images: 87.56%\n",
      "Test accuracy for 1000 images: 87.00%\n",
      "Test accuracy for 1100 images: 87.00%\n",
      "Test accuracy for 1200 images: 86.58%\n",
      "Test accuracy for 1300 images: 86.08%\n",
      "Test accuracy for 1400 images: 86.14%\n",
      "Test accuracy for 1500 images: 85.80%\n",
      "Test accuracy for 1600 images: 86.00%\n",
      "Test accuracy for 1700 images: 86.06%\n",
      "Test accuracy for 1800 images: 86.00%\n",
      "Test accuracy for 1900 images: 86.16%\n",
      "Test accuracy for 2000 images: 85.90%\n",
      "Test accuracy for 2100 images: 85.71%\n",
      "Test accuracy for 2200 images: 85.77%\n",
      "Test accuracy for 2300 images: 86.00%\n",
      "Test accuracy for 2400 images: 86.12%\n",
      "Test accuracy for 2500 images: 86.16%\n",
      "Test accuracy for 2600 images: 86.12%\n",
      "Test accuracy for 2700 images: 86.04%\n",
      "Test accuracy for 2800 images: 86.18%\n",
      "Test accuracy for 2900 images: 86.45%\n",
      "Test accuracy for 3000 images: 86.47%\n",
      "Test accuracy for 3100 images: 86.61%\n",
      "Test accuracy for 3200 images: 86.59%\n",
      "Test accuracy for 3300 images: 86.79%\n",
      "Test accuracy for 3400 images: 86.85%\n",
      "Test accuracy for 3500 images: 86.89%\n",
      "Test accuracy for 3600 images: 86.72%\n",
      "Test accuracy for 3700 images: 86.86%\n",
      "Test accuracy for 3800 images: 86.79%\n",
      "Test accuracy for 3900 images: 86.62%\n",
      "Test accuracy for 4000 images: 86.52%\n",
      "Test accuracy for 4100 images: 86.54%\n",
      "Test accuracy for 4200 images: 86.57%\n",
      "Test accuracy for 4300 images: 86.44%\n",
      "Test accuracy for 4400 images: 86.43%\n",
      "Test accuracy for 4500 images: 86.36%\n",
      "Test accuracy for 4600 images: 86.35%\n",
      "Test accuracy for 4700 images: 86.47%\n",
      "Test accuracy for 4800 images: 86.52%\n",
      "Test accuracy for 4900 images: 86.41%\n",
      "Test accuracy for 5000 images: 86.42%\n",
      "Test accuracy for 5100 images: 86.49%\n",
      "Test accuracy for 5200 images: 86.62%\n",
      "Test accuracy for 5300 images: 86.77%\n",
      "Test accuracy for 5400 images: 86.96%\n",
      "Test accuracy for 5500 images: 87.13%\n",
      "Test accuracy for 5600 images: 87.29%\n",
      "Test accuracy for 5700 images: 87.32%\n",
      "Test accuracy for 5800 images: 87.36%\n",
      "Test accuracy for 5900 images: 87.42%\n",
      "Test accuracy for 6000 images: 87.42%\n",
      "Test accuracy for 6100 images: 87.38%\n",
      "Test accuracy for 6200 images: 87.42%\n",
      "Test accuracy for 6300 images: 87.62%\n",
      "Test accuracy for 6400 images: 87.75%\n",
      "Test accuracy for 6500 images: 87.82%\n",
      "Test accuracy for 6600 images: 87.77%\n",
      "Test accuracy for 6700 images: 87.82%\n",
      "Test accuracy for 6800 images: 87.81%\n",
      "Test accuracy for 6900 images: 87.94%\n",
      "Test accuracy for 7000 images: 88.03%\n",
      "Test accuracy for 7100 images: 88.18%\n",
      "Test accuracy for 7200 images: 88.26%\n",
      "Test accuracy for 7300 images: 88.34%\n",
      "Test accuracy for 7400 images: 88.47%\n",
      "Test accuracy for 7500 images: 88.43%\n",
      "Test accuracy for 7600 images: 88.50%\n",
      "Test accuracy for 7700 images: 88.58%\n",
      "Test accuracy for 7800 images: 88.67%\n",
      "Test accuracy for 7900 images: 88.58%\n",
      "Test accuracy for 8000 images: 88.59%\n",
      "Test accuracy for 8100 images: 88.59%\n",
      "Test accuracy for 8200 images: 88.71%\n",
      "Test accuracy for 8300 images: 88.75%\n",
      "Test accuracy for 8400 images: 88.81%\n",
      "Test accuracy for 8500 images: 88.85%\n",
      "Test accuracy for 8600 images: 88.93%\n",
      "Test accuracy for 8700 images: 89.05%\n",
      "Test accuracy for 8800 images: 89.17%\n",
      "Test accuracy for 8900 images: 89.28%\n",
      "Test accuracy for 9000 images: 89.38%\n",
      "Test accuracy for 9100 images: 89.36%\n",
      "Test accuracy for 9200 images: 89.46%\n",
      "Test accuracy for 9300 images: 89.51%\n",
      "Test accuracy for 9400 images: 89.61%\n",
      "Test accuracy for 9500 images: 89.62%\n",
      "Test accuracy for 9600 images: 89.68%\n",
      "Test accuracy for 9700 images: 89.63%\n",
      "Test accuracy for 9800 images: 89.49%\n",
      "Test accuracy for 9900 images: 89.39%\n",
      "Test accuracy for 10000 images: 89.35%\n"
     ]
    }
   ],
   "source": [
    "linear.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28*28)\n",
    "        \n",
    "        outputs = linear(imgs)\n",
    "        \n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        total += imgs.size(0)\n",
    "        correct += (labels == argmax).sum().item()\n",
    "        \n",
    "        print('Test accuracy for {} images: {:.2f}%'.format(total, correct / total*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
