{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db63c637",
   "metadata": {},
   "source": [
    "### Q1. 본격적으로 Numpy와 친해지기 위해서 다양한 연산을 연습해볼 예정입니다. 무작위의 데이터를 가 진 5x3의 행렬을 가지는 numpy array와 3x2 행렬을 가지는 numpy array를 만든 후 행열곱 연산을 진행해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d84c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abf5b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46436202 0.8290724 ]\n",
      " [0.33654062 0.59001803]\n",
      " [0.34223465 0.76905414]\n",
      " [0.66833817 1.25343918]\n",
      " [0.61517246 0.98588371]] (5, 2)\n",
      "[[0.46436202 0.8290724 ]\n",
      " [0.33654062 0.59001803]\n",
      " [0.34223465 0.76905414]\n",
      " [0.66833817 1.25343918]\n",
      " [0.61517246 0.98588371]] (5, 2)\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.random.rand(5,3) # 5x3 무작위 행렬 데이터 생성\n",
    "arr2 = np.random.rand(3,2) # 3x2 무작위 행렬 데이터 생성\n",
    "\n",
    "dot = arr1 @ arr2    # 행렬의 곱 방법1\n",
    "dot_1 = arr1.dot(arr2) # 행렬의 곱 방법2\n",
    "\n",
    "print(dot, dot.shape)\n",
    "print(dot_1, dot_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fce322",
   "metadata": {},
   "source": [
    "### Q2. 두번째로는 numpy에서 자주 사용하는 concatenate 연산에 대한 미션을 수행해보겠습니다. 이제 Numpy에서 사용하는 2개의 numpy array가 있을때, 두 numpy array의 concatenate 연산을 구해보세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ea6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([[5,7], [9,11]], float)\n",
    "arr2 = np.array([[2,4], [6,8]], float)\n",
    "\n",
    "concat_1 = np.concatenate((arr1,arr2), axis = 0) # 세로로 합쳐준다\n",
    "concat_2 = np.concatenate((arr1,arr2), axis = 1) # 가로로 합쳐준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd4963d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  7.]\n",
      " [ 9. 11.]\n",
      " [ 2.  4.]\n",
      " [ 6.  8.]]\n",
      "[[ 5.  7.  2.  4.]\n",
      " [ 9. 11.  6.  8.]]\n"
     ]
    }
   ],
   "source": [
    "print(concat_1)\n",
    "print(concat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af78a2",
   "metadata": {},
   "source": [
    "### Q3. 3번부터 5번까지의 미션는 Numpy를 이용해서 정답값을 예측해보는 linear regression을 구현해 보는 미션입니다. 첫번째 단계로 데이터를 준비해보도록 하겠습니다. 아래와 같이 데이터가 주어져있을 때, 경사하강법을 위한 데이터를 분리해보세요.\n",
    "\n",
    "* 주어진 xy 데이터를 이용해서 학습과 정답 데이터를 준비해보세요.\n",
    "*추가 수정* 문제에서 주어진 xy에서 대괄호를 한 번 더 묶어주어야 문제 해결이 가능합니다.\n",
    "(차원 관련)  (np.array([[1., 2., 3., 4., 5., 6.], [10., 20., 30., 40., 50., 60.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7848acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6.] (6,)\n",
      "[10. 20. 30. 40. 50. 60.] (6,)\n"
     ]
    }
   ],
   "source": [
    "xy = np.array([[1., 2., 3., 4., 5., 6.], [10., 20., 30., 40., 50., 60.]])\n",
    "\n",
    "x_train = xy[0] # 배열 첫번째 항 Slicing\n",
    "y_train = xy[1] # 배열 두번째 항 Slicing\n",
    "\n",
    "print(x_train, x_train.shape)\n",
    "print(y_train, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d9b131",
   "metadata": {},
   "source": [
    "### Q4. 경사 하강법 구현을 위해서 위에서 분리한 x_train 데이터와 계산될 weight, bias 값을 정의해보세요. 여기서 구현한 weight와 bias 는 linear regression 계산을 진행할시 직선의 기울기와 y 절편의 값이 됩니다.\n",
    "\n",
    "* numpy 내의 random 함수를 이용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da723a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.056761964943633725] [0.7117509089519729]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "beta_gd = [np.random.uniform(-1,1)] # (-1,1) 사이 난수 설정\n",
    "bias = [np.random.uniform(-1,1)]    # (-1,1) 사이 난수 설정\n",
    "    \n",
    "print(beta_gd, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b13c5c",
   "metadata": {},
   "source": [
    "### Q5. 이제 최종적으로 linear regression을 경사하강법으로 학습하는 코드를 구현해봅시다. 경사하강법 구현을 위한 학습 Loop를 구현해보세요. 최종적으로 1000회 반복했을 시에 결과를 출력하세요.\n",
    "\n",
    "* 단, Error는 차이의 제곱을 이용해서 계산해주세요.\n",
    "* Gradient 값은 우리가 예측하는 각 변수에 대한 미분값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da1130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (         0/1000) error: 231.071064, beta_gd:  -0.189263, bias:  -0.389415\n",
      "Epoch (       100/1000) error:   0.015235, beta_gd:   2.934397, bias:   0.280859\n",
      "Epoch (       200/1000) error:   0.010573, beta_gd:   2.945348, bias:   0.233975\n",
      "Epoch (       300/1000) error:   0.007338, beta_gd:   2.954471, bias:   0.194917\n",
      "Epoch (       400/1000) error:   0.005092, beta_gd:   2.962071, bias:   0.162379\n",
      "Epoch (       500/1000) error:   0.003534, beta_gd:   2.968403, bias:   0.135273\n",
      "Epoch (       600/1000) error:   0.002453, beta_gd:   2.973677, bias:   0.112692\n",
      "Epoch (       700/1000) error:   0.001702, beta_gd:   2.978072, bias:   0.093880\n",
      "Epoch (       800/1000) error:   0.001181, beta_gd:   2.981732, bias:   0.078209\n",
      "Epoch (       900/1000) error:   0.000820, beta_gd:   2.984782, bias:   0.065153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "xy = np.array([[1., 2., 3., 4., 5., 6.],[3.,6.,9.,12.,15.,18.]])\n",
    "\n",
    "x_train = np.array(xy[0]) # Q3 배열 첫번째 항 Slicing\n",
    "y_train = np.array(xy[1]) # Q3 배열 두번째 항 Slicing\n",
    "\n",
    "beta_gd = np.random.uniform(-1,1) # Q4 (-1,1) 사이 난수 설정 \n",
    "bias = np.random.uniform(-1,1)    # Q4 (-1,1) 사이 난수 설정 \n",
    " \n",
    "learning_rate = 0.01 # 학습률\n",
    "\n",
    "\n",
    "for i in range(1000): # 1000번의 시도, 가중치와 편향을 수정해가면서 미분값 0 가까이하기\n",
    "    \n",
    "    yp = beta_gd*x_train + bias # 임의의 가중치, 편향을 계수로 하는 함수값\n",
    "    \n",
    "    error = (yp-y_train)**2 # 오차 제곱\n",
    "\n",
    "    beta_gd = beta_gd-learning_rate*(x_train*(yp-y_train)).mean() # 가중치 수정\n",
    "    bias = bias-learning_rate*(yp-y_train).mean() # 편향 수정\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Epoch ({:10d}/1000) error: {:10f}, beta_gd: {:10f}, bias: {:10f}'.format(i, error.mean(), beta_gd.item(), bias.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348e56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
